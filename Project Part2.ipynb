{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87286bcb-27c5-4f1d-ae6e-8f787b01a7f5",
   "metadata": {},
   "source": [
    "A significant amount of people now make a living through being content creator across different platforms. Among them, Youtube is known to be a major source of income. On Youtube, content creators income is highly associated with the view counts of its video. We’d like to develop a model that can help us predict future success of a video ……\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dd3dce-daa5-434c-a167-a44c236c3211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5336ff9-0e33-4d83-b818-e6d815b39239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-api-python-client in /opt/miniconda3/lib/python3.12/site-packages (2.147.0)\n",
      "Requirement already satisfied: pandas in /opt/miniconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /opt/miniconda3/lib/python3.12/site-packages (from google-api-python-client) (0.22.0)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /opt/miniconda3/lib/python3.12/site-packages (from google-api-python-client) (2.35.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /opt/miniconda3/lib/python3.12/site-packages (from google-api-python-client) (0.2.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /opt/miniconda3/lib/python3.12/site-packages (from google-api-python-client) (2.20.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /opt/miniconda3/lib/python3.12/site-packages (from google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/miniconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/miniconda3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.65.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /opt/miniconda3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (5.28.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/miniconda3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.24.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /opt/miniconda3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/miniconda3/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/miniconda3/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/miniconda3/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (4.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/miniconda3/lib/python3.12/site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client) (3.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/miniconda3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2024.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-api-python-client pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9f6f248-b011-44fc-82bf-39cceb339d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import googleapiclient.discovery\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cec2a965-d2aa-4b50-bb5d-aa3a8e83f59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's where I'm putting my YouTube API key. \n",
    "# (pls don't share it with others i have number limits every day on the videos i can pull off)\n",
    "API_KEY = \"AIzaSyCWxTrW6o2KWs9E6YH85EtDl-cURyIor4U\"  \n",
    "\n",
    "# Now we initialize the YouTube API client with the API key so we can make requests\n",
    "youtube = googleapiclient.discovery.build(\"youtube\", \"v3\", developerKey=API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a45d9bc5-04af-49ab-8feb-1a1856eeb9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, let's write a function to fetch videos based on a search query\n",
    "def get_videos(query, max_results=50, published_after=None, next_page_token=None):\n",
    "    \"\"\"\n",
    "    This function will fetch videos from YouTube based on the query we pass in.\n",
    "    \n",
    "    Parameters:\n",
    "        query (str): The search term to find relevant videos.\n",
    "        max_results (int): The maximum number of videos to get in one request.\n",
    "        published_after (str): Only fetch videos published after this timestamp (formatted in RFC 3339).\n",
    "        next_page_token (str): Here we use pagination to fetch more videos, if available.\n",
    "    \n",
    "    Returns:\n",
    "        response (dict): This will return the YouTube response with video data.\n",
    "    \"\"\"\n",
    "    request = youtube.search().list(\n",
    "        q=query,  # Here we set the search term\n",
    "        part=\"snippet\",  # Now we can get the basic info about each video (like title, channel, etc.)\n",
    "        type=\"video\",  # This will get only videos (not channels or playlists)\n",
    "        maxResults=max_results,  # This controls how many videos we’re requesting in this call\n",
    "        publishedAfter=published_after,  # Only fetch videos after a certain date\n",
    "        order=\"date\",  # Sort by the latest videos\n",
    "        pageToken=next_page_token  # If there are more pages, this will help us get the next set of results\n",
    "    )\n",
    "    response = request.execute()  # Now we execute the request to YouTube\n",
    "    return response  # Here we return the fetched video data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "617e46f4-46d3-40a1-816d-f979b974374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can write a function to get the stats for each video (e.g., views, likes, comments)\n",
    "def get_video_statistics(video_ids):\n",
    "    \"\"\"\n",
    "    This function will grab all the stats for the videos we found, including category information.\n",
    "    \n",
    "    Parameters:\n",
    "        video_ids (list): A list of video IDs for which to get statistics.\n",
    "    \n",
    "    Returns:\n",
    "        response (dict): This returns the video statistics and metadata from YouTube.\n",
    "    \"\"\"\n",
    "    request = youtube.videos().list(\n",
    "        part=\"statistics, snippet\",  # Now we can fetch both statistics (views, likes) and snippet (title, category)\n",
    "        id=\",\".join(video_ids)  # Here we join the video IDs into a single string separated by commas\n",
    "    )\n",
    "    response = request.execute()  # Now we execute the request to get the stats\n",
    "    return response  # Finally, we return the response containing video stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ca74d3d-f3b4-47ae-bf05-b9f95da0531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the main function that collects video data up to a maximum of 50 videos\n",
    "def collect_video_data(query, max_results_total=50, max_results_per_page=50):\n",
    "    \"\"\"\n",
    "    Here we collect video data for a specific search query, collecting up to max_results_total videos\n",
    "    published within the last 24 hours.\n",
    "    \n",
    "    Parameters:\n",
    "        query (str): The search query to find relevant videos.\n",
    "        max_results_total (int): The total number of videos to collect.\n",
    "        max_results_per_page (int): Maximum videos to fetch per request.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Now we’ll return a DataFrame with all the collected video statistics and metadata.\n",
    "    \"\"\"\n",
    "    video_data = []\n",
    "    total_videos_collected = 0  # Track the total number of videos collected\n",
    "    next_page_token = None  # Initialize the page token for pagination\n",
    "    \n",
    "    # Get the current time and subtract 24 hours to get the time range for the last 24 hours\n",
    "    current_time = datetime.now(timezone.utc)\n",
    "    published_after = (current_time - timedelta(days=1)).strftime(\"%Y-%m-%dT%H:%M:%SZ\")  # 24 hours ago in RFC 3339 format\n",
    "    \n",
    "    # Loop until we reach max_results_total videos\n",
    "    while total_videos_collected < max_results_total:\n",
    "        # Fetch videos published within the last 24 hours\n",
    "        response = get_videos(query, max_results=max_results_per_page, published_after=published_after, next_page_token=next_page_token)\n",
    "        \n",
    "        video_ids = [item['id']['videoId'] for item in response.get('items', [])]  # Extract video IDs from the response\n",
    "        \n",
    "        if not video_ids:\n",
    "            break  # If no more videos are found, we stop the loop\n",
    "        \n",
    "        # Get statistics for the fetched videos\n",
    "        stats_response = get_video_statistics(video_ids)\n",
    "        \n",
    "        # Loop through each video and collect the data\n",
    "        for item in stats_response.get('items', []):\n",
    "            video_id = item['id']\n",
    "            statistics = item['statistics']  # Get the statistics like views, likes, etc.\n",
    "            snippet = item['snippet']  # Get the metadata like title, category, etc.\n",
    "            \n",
    "            # Append the collected data for each video\n",
    "            video_data.append({\n",
    "                'Video ID': video_id,\n",
    "                'View Count': int(statistics.get('viewCount', 0)),  # Ensure view count is an integer, defaulting to 0 if missing\n",
    "                'Like Count': int(statistics.get('likeCount', 0)),  # Same for like count\n",
    "                'Comment Count': int(statistics.get('commentCount', 0)),  # Same for comment count\n",
    "                'Favorite Count': int(statistics.get('favoriteCount', 0)),  # Handle favorite count similarly\n",
    "                'Video Category': snippet.get('categoryId', 'Unknown'),  # Store the category ID as a feature\n",
    "                'Video Published At': snippet.get('publishedAt', 'Unknown')  # Store the publication date\n",
    "            })\n",
    "        \n",
    "        # Update the total number of videos collected\n",
    "        total_videos_collected += len(video_ids)\n",
    "        \n",
    "        # Check if we have reached the max results or no more pages to fetch\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token or total_videos_collected >= max_results_total:\n",
    "            break\n",
    "    \n",
    "    # Finally, convert the list of video data into a DataFrame\n",
    "    return pd.DataFrame(video_data[:max_results_total])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08c06930-da89-4c03-8794-7b3989f3c4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collection complete. Saved to YP_Youtube_50_Videos2.csv\n"
     ]
    }
   ],
   "source": [
    "# Now we can use all of this to collect data and save it to a CSV\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"Youtube\"  # Set the search term to something relevant \n",
    "    max_videos = 50  # Collect up to 50 videos in total\n",
    "    \n",
    "    # Time to collect our video data\n",
    "    df = collect_video_data(query, max_results_total=max_videos)\n",
    "    \n",
    "    # Now we save the collected data to a CSV file\n",
    "    df.to_csv('YP_Youtube_50_Videos2.csv', index=False)\n",
    "    print(\"Data collection complete. Saved to YP_Youtube_50_Videos2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cea74d82-71e9-4ec1-9449-83d734064ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category mapping completed. Saved to YP_Youtube_50_Videos2.csv\n"
     ]
    }
   ],
   "source": [
    "# Now, we're going to create a dictionary to map those category IDs to actual category names. \n",
    "# This way, we can easily understand what each category represents.\n",
    "category_mapping = {\n",
    "    '1': 'Film & Animation',\n",
    "    '2': 'Autos & Vehicles',\n",
    "    '10': 'Music',\n",
    "    '15': 'Pets & Animals',\n",
    "    '17': 'Sports',\n",
    "    '18': 'Short Movies',\n",
    "    '19': 'Travel & Events',\n",
    "    '20': 'Gaming',\n",
    "    '21': 'Videoblogging',\n",
    "    '22': 'People & Blogs',\n",
    "    '23': 'Comedy',\n",
    "    '24': 'Entertainment',\n",
    "    '25': 'News & Politics',\n",
    "    '26': 'Howto & Style',\n",
    "    '27': 'Education',\n",
    "    '28': 'Science & Technology',\n",
    "    '29': 'Nonprofits & Activism'\n",
    "    # More categories can be added here if needed, but this should cover the most common ones. I didn't find a better list/map yet if  you do you  can add here\n",
    "}\n",
    "\n",
    "# Now let's load the CSV file that contains our YouTube video data.\n",
    "# This file has all the video details that we collected earlier.\n",
    "df = pd.read_csv('YP_Youtube_50_Videos2.csv')\n",
    "\n",
    "# Here's where the transformation happens. We're going to replace the category IDs with the actual names.\n",
    "# First, we make sure the 'Video Category' column is in string format, then we map those IDs to the category names.\n",
    "df['Video Category'] = df['Video Category'].astype(str).map(category_mapping)\n",
    "\n",
    "# Now that we've updated the data, let's save it to a new CSV file.\n",
    "# This way, the original data stays untouched, and we get a more readable version with category names.\n",
    "df.to_csv('YP_Youtube_50_Videos2.csv', index=False)\n",
    "\n",
    "# Finally, let's print a message to confirm that everything worked and the file was saved successfully.\n",
    "print(\"Category mapping completed. Saved to YP_Youtube_50_Videos2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6426d7cb-04cc-4755-8375-e593b19717a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Video ID  View Count  Like Count  Comment Count  Favorite Count  \\\n",
      "0   mb5ldcr1jbA      266046        1263              0               0   \n",
      "1   rtLGwJF9Jxc       88604         557              0               0   \n",
      "2   HAOgdVdSSkA      576583           0              6               0   \n",
      "3   WWeOBvAvwrY       21829        4059            532               0   \n",
      "4   WunqF6zQnbI       31961        1640             16               0   \n",
      "5   rm4qj0SJKKE        6134        1203              0               0   \n",
      "6   uDvNq7vtGnM        4031         164             28               0   \n",
      "7   p8SIBaDl9PE       69614         897              0               0   \n",
      "8   q_V96gv_IDA       93868        1267              0               0   \n",
      "9   TS6wQl2Gq_Y       11103         497              1               0   \n",
      "10  CQMZndfGRWI        8241        1267              0               0   \n",
      "11  qX6MAp5qQrU      106761        6819             69               0   \n",
      "12  tcAExHTclc4        8824          68              0               0   \n",
      "13  95RPXz_h2Fw       11241        2528            145               0   \n",
      "14  6kKE4Rurmmk        6098         657             65               0   \n",
      "15  YRyo5vL6XsI       36363        5316             77               0   \n",
      "16  Y8hZ5i0TGIs      144189        1117            134               0   \n",
      "17  JrD7QKy8PUI         353          19              1               0   \n",
      "18  f7v8WJgChU8         408          10              0               0   \n",
      "19  zToiY455lhQ      493219      111578           2395               0   \n",
      "20  uDVNRYxIMRE       72765         351              0               0   \n",
      "21  Yfcs9OPrQz8      108377         695              0               0   \n",
      "22  -n0waD4qzI4       10969         373              6               0   \n",
      "23  qfih0r0sq0c       27266        1073              7               0   \n",
      "24  NUzGaZkGTu4       79959         395              0               0   \n",
      "25  I1cPbwuMc3U      241020         635              2               0   \n",
      "26  3s6uanj_5DY        8515         655             65               0   \n",
      "27  XzW21Dj6PIQ        1771         193              3               0   \n",
      "28  bD-R-2DPzuc      518760       14171            215               0   \n",
      "29  f-Dvm17aVxk        6978         275             33               0   \n",
      "30  RzQpDaa2044     2186425       31280             33               0   \n",
      "31  Gs1r0uzedBw        1140         178              2               0   \n",
      "32  F4oaCRpuCtU       43216         323             46               0   \n",
      "33  ng_xV81XhHM        6304          35             56               0   \n",
      "34  yGLEpVbJm40      875459       27560           6858               0   \n",
      "35  lKwo4S2VA3c       41612         243              3               0   \n",
      "36  T0sPOtCwXio         955         255             14               0   \n",
      "37  6iG4Zp8HLOA      212067        4363              0               0   \n",
      "38  E-TA8hJIUk8        6616         277             16               0   \n",
      "39  gE4eK3He-7w       73805        3099              0               0   \n",
      "40  qWDt3sCTGFg       41209        2452             82               0   \n",
      "41  zZsgnPmP6lg       36141        3713             19               0   \n",
      "42  PYJ4do-OZAQ       72905        1508              0               0   \n",
      "43  _AlZYvis4uE      163845        1752              0               0   \n",
      "44  Nks9y_kpofE       24171         357              2               0   \n",
      "45  sMY2RlXdreo       28449        4515            164               0   \n",
      "46  u5us4Y5Q4AY        2612         620              8               0   \n",
      "47  jX9vfyAyELE         529           6              1               0   \n",
      "48  IXq9Ex6qLxY       21789        1084             87               0   \n",
      "49  00-9NCfhVF8       12171        1629             27               0   \n",
      "\n",
      "           Video Category    Video Published At  \n",
      "0         News & Politics  2024-10-21T19:10:34Z  \n",
      "1        Film & Animation  2024-10-21T18:45:33Z  \n",
      "2         News & Politics  2024-10-21T18:41:24Z  \n",
      "3         News & Politics  2024-10-21T18:31:26Z  \n",
      "4         News & Politics  2024-10-21T18:10:08Z  \n",
      "5         News & Politics  2024-10-21T17:56:34Z  \n",
      "6           Entertainment  2024-10-21T17:38:38Z  \n",
      "7         News & Politics  2024-10-21T17:37:58Z  \n",
      "8         News & Politics  2024-10-21T17:34:17Z  \n",
      "9                  Gaming  2024-10-21T17:24:21Z  \n",
      "10         People & Blogs  2024-10-21T17:08:39Z  \n",
      "11        News & Politics  2024-10-21T16:56:12Z  \n",
      "12                 Sports  2024-10-21T16:22:16Z  \n",
      "13         People & Blogs  2024-10-21T16:13:57Z  \n",
      "14                 Comedy  2024-10-21T16:11:11Z  \n",
      "15          Entertainment  2024-10-21T16:07:27Z  \n",
      "16        News & Politics  2024-10-21T15:52:42Z  \n",
      "17              Education  2024-10-21T15:51:16Z  \n",
      "18                 Sports  2024-10-21T15:30:18Z  \n",
      "19         People & Blogs  2024-10-21T15:06:25Z  \n",
      "20       Film & Animation  2024-10-21T15:00:59Z  \n",
      "21        News & Politics  2024-10-21T15:01:13Z  \n",
      "22          Entertainment  2024-10-21T15:00:06Z  \n",
      "23                 Gaming  2024-10-21T14:44:32Z  \n",
      "24       Film & Animation  2024-10-21T14:45:49Z  \n",
      "25        News & Politics  2024-10-21T14:29:42Z  \n",
      "26                 Comedy  2024-10-21T14:26:44Z  \n",
      "27                 Sports  2024-10-21T14:15:02Z  \n",
      "28        News & Politics  2024-10-21T14:05:31Z  \n",
      "29                 Sports  2024-10-21T14:00:42Z  \n",
      "30                 Sports  2024-10-21T13:32:11Z  \n",
      "31              Education  2024-10-21T13:31:20Z  \n",
      "32        News & Politics  2024-10-21T13:28:10Z  \n",
      "33        News & Politics  2024-10-21T13:03:08Z  \n",
      "34        News & Politics  2024-10-21T13:01:10Z  \n",
      "35        News & Politics  2024-10-21T13:01:47Z  \n",
      "36                 Gaming  2024-10-21T12:55:26Z  \n",
      "37        News & Politics  2024-10-21T12:52:07Z  \n",
      "38                 Sports  2024-10-21T12:40:42Z  \n",
      "39        News & Politics  2024-10-21T12:39:16Z  \n",
      "40          Entertainment  2024-10-21T12:30:46Z  \n",
      "41                 Gaming  2024-10-21T12:16:34Z  \n",
      "42          Entertainment  2024-10-21T12:11:16Z  \n",
      "43          Entertainment  2024-10-21T12:09:45Z  \n",
      "44                 Gaming  2024-10-21T12:08:52Z  \n",
      "45                  Music  2024-10-21T12:01:12Z  \n",
      "46  Nonprofits & Activism  2024-10-21T12:01:11Z  \n",
      "47          Entertainment  2024-10-21T12:00:06Z  \n",
      "48         People & Blogs  2024-10-21T11:53:17Z  \n",
      "49                  Music  2024-10-21T11:50:09Z  \n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('YP_Youtube_50_Videos2.csv')\n",
    "\n",
    "# Display the first 50 rows of the DataFrame\n",
    "print(df.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7b85fa7-db1b-434e-a164-0af2d5cf871e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mb5ldcr1jbA', 'rtLGwJF9Jxc', 'HAOgdVdSSkA', 'WWeOBvAvwrY', 'WunqF6zQnbI', 'rm4qj0SJKKE', 'uDvNq7vtGnM', 'p8SIBaDl9PE', 'q_V96gv_IDA', 'TS6wQl2Gq_Y', 'CQMZndfGRWI', 'qX6MAp5qQrU', 'tcAExHTclc4', '95RPXz_h2Fw', '6kKE4Rurmmk', 'YRyo5vL6XsI', 'Y8hZ5i0TGIs', 'JrD7QKy8PUI', 'f7v8WJgChU8', 'zToiY455lhQ', 'uDVNRYxIMRE', 'Yfcs9OPrQz8', '-n0waD4qzI4', 'qfih0r0sq0c', 'NUzGaZkGTu4', 'I1cPbwuMc3U', '3s6uanj_5DY', 'XzW21Dj6PIQ', 'bD-R-2DPzuc', 'f-Dvm17aVxk', 'RzQpDaa2044', 'Gs1r0uzedBw', 'F4oaCRpuCtU', 'ng_xV81XhHM', 'yGLEpVbJm40', 'lKwo4S2VA3c', 'T0sPOtCwXio', '6iG4Zp8HLOA', 'E-TA8hJIUk8', 'gE4eK3He-7w', 'qWDt3sCTGFg', 'zZsgnPmP6lg', 'PYJ4do-OZAQ', '_AlZYvis4uE', 'Nks9y_kpofE', 'sMY2RlXdreo', 'u5us4Y5Q4AY', 'jX9vfyAyELE', 'IXq9Ex6qLxY', '00-9NCfhVF8']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the function to extract video IDs from the CSV with an optional parameter to limit the number of rows\n",
    "def get_video_ids_from_csv(file_path, n=None):\n",
    "    \"\"\"\n",
    "    Extract the first n video IDs from a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the CSV file.\n",
    "        n (int, optional): Number of rows to extract. If None, extract all rows.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of video IDs.\n",
    "    \"\"\"\n",
    "    # Load the CSV into a pandas DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # If n is provided, extract only the first n rows, otherwise extract all rows\n",
    "    if n:\n",
    "        video_ids = df['Video ID'].head(n).tolist()  # Extract first n rows\n",
    "    else:\n",
    "        video_ids = df['Video ID'].tolist()  # Extract all rows if n is not provided\n",
    "    \n",
    "    return video_ids\n",
    "\n",
    "\n",
    "csv_file_path = '/Users/yipengandrewwang/DS3000-3/Project1/YP_Youtube_50_Videos2.csv'\n",
    "\n",
    "#Extract only the first 10 video IDs\n",
    "video_ids_list = get_video_ids_from_csv(csv_file_path, n=50)\n",
    "print(video_ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3544d2b3-f861-4a8b-bb82-9912203ef4f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(video_ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad407830-e998-4d92-b1d1-4b041f40a082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import googleapiclient.discovery\n",
    "\n",
    "# Initialize YouTube API client\n",
    "API_KEY = \"AIzaSyCWxTrW6o2KWs9E6YH85EtDl-cURyIor4U\"  \n",
    "youtube = googleapiclient.discovery.build(\"youtube\", \"v3\", developerKey=API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eccec725-14f9-488e-bfea-3ddda97ad71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated Function to fetch video statistics for a list of video IDs\n",
    "def get_video_statistics(video_ids):\n",
    "    \"\"\"\n",
    "    This function fetches video statistics like views, likes, comments for a list of video IDs.\n",
    "    \n",
    "    Parameters:\n",
    "        video_ids (list): A list of video IDs for which to get statistics.\n",
    "    \n",
    "    Returns:\n",
    "        response (dict): The YouTube response containing the video statistics.\n",
    "    \"\"\"\n",
    "    request = youtube.videos().list(\n",
    "        part=\"statistics, snippet\",  # Get both statistics and metadata\n",
    "        id=\",\".join(video_ids)  # Join the video IDs into a comma-separated string\n",
    "    )\n",
    "    response = request.execute()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "549abaaa-4768-46e4-88a5-b9be874ce2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main function for collecting data every 30 minutes\n",
    "def collect_consecutive_sessions_data(video_ids, sessions=10, output_file=\"YP_Youtube_Consecutive_Sessions.csv\"):\n",
    "    \"\"\"\n",
    "    Collects video statistics for the same set of videos every 30 minutes over a specified number of sessions\n",
    "    and saves the results to a CSV file after each session.\n",
    "    \n",
    "    Parameters:\n",
    "        video_ids (list): List of video IDs for which to fetch statistics.\n",
    "        sessions (int): The number of consecutive 30-minute sessions to collect data (default is 10 sessions).\n",
    "        output_file (str): The name of the output CSV file to store the collected data.\n",
    "    \n",
    "    Returns:\n",
    "        None: The data is saved to the output CSV file after each session.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a list to store the data for all sessions\n",
    "    all_sessions_data = []\n",
    "    \n",
    "    # Collect data for each video over consecutive sessions (every 30 minutes)\n",
    "    for session in range(1, sessions + 1):\n",
    "        print(f\"Collecting data for Session {session} (every 1 hours)...\")\n",
    "        session_data = []  # Store data for each video in this session\n",
    "        \n",
    "        # Fetch statistics for the same video IDs\n",
    "        stats_response = get_video_statistics(video_ids)\n",
    "        \n",
    "        # Loop through each video and collect the stats for the current session\n",
    "        for item in stats_response.get('items', []):\n",
    "            video_id = item['id']\n",
    "            statistics = item['statistics']\n",
    "            snippet = item['snippet']\n",
    "            \n",
    "            # Store video stats for the current session\n",
    "            session_data.append({\n",
    "                'Video ID': video_id,\n",
    "                'Session': session,\n",
    "                'View Count': int(statistics.get('viewCount', 0)),\n",
    "                'Like Count': int(statistics.get('likeCount', 0)),\n",
    "                'Comment Count': int(statistics.get('commentCount', 0)),\n",
    "                'Favorite Count': int(statistics.get('favoriteCount', 0)),\n",
    "                'Video Category': snippet.get('categoryId', 'Unknown'),\n",
    "                'Video Published At': snippet.get('publishedAt', 'Unknown'),\n",
    "            })\n",
    "        \n",
    "        # Append the current session's data to the all_sessions_data list\n",
    "        all_sessions_data.extend(session_data)\n",
    "        \n",
    "        # Convert the collected data into a pandas DataFrame\n",
    "        df = pd.DataFrame(session_data)\n",
    "        \n",
    "        # Save the data to CSV file after each session (appending if it exists)\n",
    "        if session == 1:\n",
    "            # For the first session, write a new CSV file with the header\n",
    "            df.to_csv(output_file, mode='w', header=True, index=False)\n",
    "        else:\n",
    "            # For subsequent sessions, append to the existing file without writing the header\n",
    "            df.to_csv(output_file, mode='a', header=False, index=False)\n",
    "        \n",
    "        # Print summary of collected data for this session\n",
    "        print(f\"Session {session} data collected. Collected data for {len(session_data)} videos.\")\n",
    "        print(f\"Waiting 1 hours before the next collection...\")\n",
    "        \n",
    "        # Sleep for 3 hours before collecting the next session's data\n",
    "        time.sleep(1 * 60 * 60)  # Sleep for 1 hours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5478b962-7631-43c7-a78e-d74a3fa042d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data for Session 1 (every 1 hours)...\n",
      "Session 1 data collected. Collected data for 50 videos.\n",
      "Waiting 1 hours before the next collection...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  \n",
    "    sessions_to_collect = 30  # Collect data for 18 consecutive 2-hour sessions\n",
    "    \n",
    "    # Collect video data over consecutive sessions for the provided video IDs\n",
    "    collect_consecutive_sessions_data(video_ids_list, sessions=sessions_to_collect, output_file=\"YP_Youtube_Consecutive_Sessions4.3.csv\")\n",
    "    print(\"Data collection for consecutive 1-hour sessions complete. Saved to YP_Youtube_Consecutive_Sessions_4.3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ae758d-0a74-4e76-b8d5-dd95b8513f16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7323d521-b963-4499-892b-c56469683674",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69456bf8-856a-4a33-b8d3-41a35f65edff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('YP_Youtube1_4.csv',header=0)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e692f55e-ce97-4f92-bbd1-acc2b0bf5066",
   "metadata": {},
   "outputs": [],
   "source": [
    "len = df.shape[0]\n",
    "for i in range(len):\n",
    "    df.loc[i,'Video Published At'] = convert_to_dt_obj(df['Video Published At'][i])\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fbb808-32ef-42d0-a893-7ab084dfd42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df['Video Published At'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb9795a-4a84-49a4-9792-48a1b8cee8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted = df.sort_values(by='Video Published At',ascending=True)\n",
    "sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8d5eb5-f89d-47f9-bd46-804c8f3da38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_mapping = {\n",
    "#     '1': 'Film & Animation',\n",
    "#     '2': 'Autos & Vehicles',\n",
    "#     '10': 'Music',\n",
    "#     '15': 'Pets & Animals',\n",
    "#     '17': 'Sports',\n",
    "#     '18': 'Short Movies',\n",
    "#     '19': 'Travel & Events',\n",
    "#     '20': 'Gaming',\n",
    "#     '21': 'Videoblogging',\n",
    "#     '22': 'People & Blogs',\n",
    "#     '23': 'Comedy',\n",
    "#     '24': 'Entertainment',\n",
    "#     '25': 'News & Politics',\n",
    "#     '26': 'Howto & Style',\n",
    "#     '27': 'Education',\n",
    "#     '28': 'Science & Technology',\n",
    "#     '29': 'Nonprofits & Activism'\n",
    "#     # More categories can be added here if needed, but this should cover the most common ones. I didn't find a better list/map yet if  you do you  can add here\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8754e02f-b2ea-4b25-8a00-13151d4ade70",
   "metadata": {},
   "outputs": [],
   "source": [
    "recreation = ['Film & Animation','Music','Pets & Animals','Sports','Short Movies','Travel & Events',\\\n",
    "              'Gaming''Videoblogging','People & Blogs','Comedy','Entertainment'\n",
    "                ]\n",
    "\n",
    "info = ['News & Politics','Howto & Style','Education','Science & Technology','Nonprofits & Activism']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d688ef9c-f5b9-405c-8795-fba00da8fe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_categorize(string,recreation_list):\n",
    "    if string in recreation_list:\n",
    "        return 'Recreation'\n",
    "    else:\n",
    "        return 'Info'\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len):\n",
    "    df.loc[i,'Video Category'] = re_categorize(df['Video Category'][i],recreation)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa28c0f-1482-4956-a05b-8c342446b1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.groupby(['Video Category'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4a1b97-2eb9-4d58-afc5-a52775249be7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66086bcd-12e0-4b3b-83db-c24663324572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
