{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e58fe91-3e39-4710-a18d-e04a7a8ddadf",
   "metadata": {},
   "source": [
    "# Project Proposal: Predicting the Popularity of YouTube Videos Using Early Metrics\n",
    "\n",
    "## 1. Problem Description and Motivation (2%)\n",
    "\n",
    "We want to predict how popular YouTube videos will be using early engagement metrics. YouTube's algorithm usually recommends videos based on early stats like views, likes, comments, and shares. But figuring out *which specific factors* have the biggest impact on whether a video goes viral can offer important insights for creators, marketers, and viewers.\n",
    "\n",
    "**Motivation**: With billions of videos on YouTube, knowing which ones will become popular early on can be super helpful for improving content strategy. This project will help people (and me! i have a pathetic youtube channel that nobody seems to notice) predict whether a video will be a hit by analyzing early data (like view count) .\n",
    "\n",
    "This is my channel i hope my project will help it one day :( ..... \n",
    "https://www.youtube.com/channel/UC91T6l13DPKKHhLMF10Gi2g?sub_confirmation=1\n",
    "\n",
    "We’re focusing on two key questions:\n",
    "1. **Which early engagement metrics (views, likes, comments) have the biggest influence on a video’s future success?**\n",
    "2. **Can we accurately predict the future view count of a video using data from its first 24 hours?**\n",
    "\n",
    "This project taps into the growing role of video content in today’s digital world and could be a valuable tool for content creators looking to improve their strategies.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474b40d1-1622-4fb0-8544-1e1b3e4f68c7",
   "metadata": {},
   "source": [
    "## 2. Data Source and Collection Plan (2%)\n",
    "\n",
    "For this project, we’ll gather data using the **YouTube Data API**(very unfortunately it only allows us to collect `1500` videos per day. That's why i've consecutively collected datas for the past week so if you pick my proposal we will start with about at least `4500` videos. yay!). We’ll focus on videos uploaded in the last 30 days from a specific category (e.g., \"Data Science Tutorial\") to keep the data fresh and relevant.\n",
    "\n",
    "**Features we’ll collect:**\n",
    "- **Numeric Features**: View Count, Like Count, Comment Count\n",
    "- **Categorical Feature**: Video Category (e.g., \"Education,\" \"Science & Technology\")\n",
    "\n",
    "**Data Collection**: \n",
    "We’ll use Python and the YouTube Data API to collect the data. Each API call will retrieve up to 50 videos, and we’ll paginate through the results if needed. The data will be focused on engagement metrics ( we may later decide if we want the data to be collected only from the first 24 hours after the video is released by changing code here:  for day in range(days):  # Now we loop through each day to collect data\n",
    "        # We need to format the start and end dates correctly for YouTube's API\n",
    "        start_date = (today - timedelta(days=day+1)).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        end_date = (today - timedelta(days=day)).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        )  We’ll map category IDs to their actual names to make the data easier to understand.\n",
    "\n",
    "We’ll store the data in a CSV file with the following columns:\n",
    "- `Video ID`: Unique identifier for each video\n",
    "- `View Count`: Number of views the video has received\n",
    "- `Like Count`: Number of likes\n",
    "- `Comment Count`: Number of comments\n",
    "- `Video Category`: The category the video belongs to\n",
    "- `Video Published At`: The date and time the video was published\n",
    "\n",
    "**I've paste my code below for ur referennce**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11381ce5-7293-424b-83f2-e369e1c8e2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-api-python-client in /opt/miniconda3/lib/python3.12/site-packages (2.147.0)\n",
      "Requirement already satisfied: pandas in /opt/miniconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /opt/miniconda3/lib/python3.12/site-packages (from google-api-python-client) (0.22.0)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /opt/miniconda3/lib/python3.12/site-packages (from google-api-python-client) (2.35.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /opt/miniconda3/lib/python3.12/site-packages (from google-api-python-client) (0.2.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /opt/miniconda3/lib/python3.12/site-packages (from google-api-python-client) (2.20.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /opt/miniconda3/lib/python3.12/site-packages (from google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/miniconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/miniconda3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.65.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /opt/miniconda3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (5.28.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/miniconda3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.24.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /opt/miniconda3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/miniconda3/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/miniconda3/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/miniconda3/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (4.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/miniconda3/lib/python3.12/site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client) (3.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/miniconda3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2024.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-api-python-client pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e535eda5-1b74-4d70-9315-3a85d45f5fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "514d2165-63f4-41b3-8392-e8a63b1abb30",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43013189-d456-4707-86f2-e64e3640a69a",
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpError",
     "evalue": "<HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/search?q=Data+Science+Tutorial&part=snippet&type=video&maxResults=50&publishedAfter=2024-10-02T23%3A54%3A17Z&order=relevance&key=AIzaSyDOg4YBSWnZkOdsb67hDOVRsHuCht3TVDg&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 131\u001b[0m\n\u001b[1;32m    128\u001b[0m max_total_results_per_day \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m150\u001b[39m  \u001b[38;5;66;03m# Fetch up to 150 videos in total per day (with pagination)\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# Time to collect our video data\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m df \u001b[38;5;241m=\u001b[39m collect_video_data(query, days\u001b[38;5;241m=\u001b[39mdays_to_collect, max_results_per_day\u001b[38;5;241m=\u001b[39mmax_videos_per_day, max_total_results_per_day\u001b[38;5;241m=\u001b[39mmax_total_results_per_day)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Now we save the collected data to a CSV file\u001b[39;00m\n\u001b[1;32m    134\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYP_Youtube1_3.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[2], line 85\u001b[0m, in \u001b[0;36mcollect_video_data\u001b[0;34m(query, days, max_results_per_day, max_total_results_per_day)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Now we loop until we've fetched the total results for the day\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m total_results_fetched \u001b[38;5;241m<\u001b[39m max_total_results_per_day:\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# Fetch the videos published after the start date\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m     response \u001b[38;5;241m=\u001b[39m get_videos(query, max_results\u001b[38;5;241m=\u001b[39mmax_results_per_day, published_after\u001b[38;5;241m=\u001b[39mstart_date, next_page_token\u001b[38;5;241m=\u001b[39mnext_page_token)\n\u001b[1;32m     87\u001b[0m     video_ids \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideoId\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m'\u001b[39m, [])]  \u001b[38;5;66;03m# Extract video IDs from the response\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m video_ids:\n",
      "Cell \u001b[0;32mIn[2], line 36\u001b[0m, in \u001b[0;36mget_videos\u001b[0;34m(query, max_results, published_after, next_page_token)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03mThis function will fetch videos from YouTube based on the query we pass in.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m    response (dict): This will return the YouTube response with video data.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m request \u001b[38;5;241m=\u001b[39m youtube\u001b[38;5;241m.\u001b[39msearch()\u001b[38;5;241m.\u001b[39mlist(\n\u001b[1;32m     28\u001b[0m     q\u001b[38;5;241m=\u001b[39mquery,  \u001b[38;5;66;03m# Here we set the search term\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     part\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnippet\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Now we can get the basic info about each video (like title, channel, etc.)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     pageToken\u001b[38;5;241m=\u001b[39mnext_page_token  \u001b[38;5;66;03m# If there are more pages, this will help us get the next set of results\u001b[39;00m\n\u001b[1;32m     35\u001b[0m )\n\u001b[0;32m---> 36\u001b[0m response \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mexecute()  \u001b[38;5;66;03m# Now we execute the request to YouTube\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/googleapiclient/_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement \u001b[38;5;241m==\u001b[39m POSITIONAL_WARNING:\n\u001b[1;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/googleapiclient/http.py:938\u001b[0m, in \u001b[0;36mHttpRequest.execute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    936\u001b[0m     callback(resp)\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpError(resp, content, uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muri)\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostproc(resp, content)\n",
      "\u001b[0;31mHttpError\u001b[0m: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/search?q=Data+Science+Tutorial&part=snippet&type=video&maxResults=50&publishedAfter=2024-10-02T23%3A54%3A17Z&order=relevance&key=AIzaSyDOg4YBSWnZkOdsb67hDOVRsHuCht3TVDg&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import googleapiclient.discovery\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "# Here's where I'm putting my YouTube API key. \n",
    "# (pls don't share it with others i have number limits every day on the videos i can pull off)\n",
    "API_KEY = \"AIzaSyDOg4YBSWnZkOdsb67hDOVRsHuCht3TVDg\"  \n",
    "\n",
    "# Now we initialize the YouTube API client with the API key so we can make requests\n",
    "youtube = googleapiclient.discovery.build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# Next, let's write a function to fetch videos based on a search query\n",
    "def get_videos(query, max_results=50, published_after=None, next_page_token=None):\n",
    "    \"\"\"\n",
    "    This function will fetch videos from YouTube based on the query we pass in.\n",
    "    \n",
    "    Parameters:\n",
    "        query (str): The search term to find relevant videos.\n",
    "        max_results (int): The maximum number of videos to get in one request.\n",
    "        published_after (str): Only fetch videos published after this timestamp (formatted in RFC 3339).\n",
    "        next_page_token (str): Here we use pagination to fetch more videos, if available.\n",
    "    \n",
    "    Returns:\n",
    "        response (dict): This will return the YouTube response with video data.\n",
    "    \"\"\"\n",
    "    request = youtube.search().list(\n",
    "        q=query,  # Here we set the search term\n",
    "        part=\"snippet\",  # Now we can get the basic info about each video (like title, channel, etc.)\n",
    "        type=\"video\",  # This will get only videos (not channels or playlists)\n",
    "        maxResults=max_results,  # This controls how many videos we’re requesting in this call\n",
    "        publishedAfter=published_after,  # Only fetch videos after a certain date\n",
    "        order=\"relevance\",  # This sorts the videos by relevance to our query\n",
    "        pageToken=next_page_token  # If there are more pages, this will help us get the next set of results\n",
    "    )\n",
    "    response = request.execute()  # Now we execute the request to YouTube\n",
    "    return response  # Here we return the fetched video data\n",
    "\n",
    "# Now we can write a function to get the stats for each video (e.g., views, likes, comments)\n",
    "def get_video_statistics(video_ids):\n",
    "    \"\"\"\n",
    "    This function will grab all the stats for the videos we found, including category information.\n",
    "    \n",
    "    Parameters:\n",
    "        video_ids (list): A list of video IDs for which to get statistics.\n",
    "    \n",
    "    Returns:\n",
    "        response (dict): This returns the video statistics and metadata from YouTube.\n",
    "    \"\"\"\n",
    "    request = youtube.videos().list(\n",
    "        part=\"statistics, snippet\",  # Now we can fetch both statistics (views, likes) and snippet (title, category)\n",
    "        id=\",\".join(video_ids)  # Here we join the video IDs into a single string separated by commas\n",
    "    )\n",
    "    response = request.execute()  # Now we execute the request to get the stats\n",
    "    return response  # Finally, we return the response containing video stats\n",
    "\n",
    "# This is the main function that collects video data over a number of days\n",
    "def collect_video_data(query, days=30, max_results_per_day=50, max_total_results_per_day=100):\n",
    "    \"\"\"\n",
    "    Here we collect video data for a specific search query over several days.\n",
    "    \n",
    "    Parameters:\n",
    "        query (str): The search query to find relevant videos.\n",
    "        days (int): The number of days to collect video data.\n",
    "        max_results_per_day (int): Maximum videos to fetch per request (per day).\n",
    "        max_total_results_per_day (int): The total number of videos to fetch per day, including paginated results.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Now we’ll return a DataFrame with all the collected video statistics and metadata.\n",
    "    \"\"\"\n",
    "    video_data = []\n",
    "    today = datetime.now(timezone.utc)  # Here we get the current date and time in UTC to work with timestamps\n",
    "    \n",
    "    for day in range(days):  # Now we loop through each day to collect data\n",
    "        # We need to format the start and end dates correctly for YouTube's API\n",
    "        start_date = (today - timedelta(days=day+1)).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        end_date = (today - timedelta(days=day)).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        \n",
    "        next_page_token = None  # Initialize the page token for pagination\n",
    "        total_results_fetched = 0  # Keep track of how many results we’ve fetched so far\n",
    "        \n",
    "        # Now we loop until we've fetched the total results for the day\n",
    "        while total_results_fetched < max_total_results_per_day:\n",
    "            # Fetch the videos published after the start date\n",
    "            response = get_videos(query, max_results=max_results_per_day, published_after=start_date, next_page_token=next_page_token)\n",
    "            \n",
    "            video_ids = [item['id']['videoId'] for item in response.get('items', [])]  # Extract video IDs from the response\n",
    "            \n",
    "            if not video_ids:\n",
    "                break  # If no videos are found, we stop for this day\n",
    "            \n",
    "            # Now we can get the stats for each video\n",
    "            stats_response = get_video_statistics(video_ids)\n",
    "            \n",
    "            # Next, we loop through each video and collect the data\n",
    "            for item in stats_response.get('items', []):\n",
    "                video_id = item['id']\n",
    "                statistics = item['statistics']  # Get the statistics like views, likes, etc.\n",
    "                snippet = item['snippet']  # Grab the metadata like title and category\n",
    "                \n",
    "                # Now we add the collected data to our list\n",
    "                video_data.append({\n",
    "                    'Video ID': video_id,\n",
    "                    'View Count': int(statistics.get('viewCount', 0)),  # Make sure view count is an integer, defaulting to 0 if missing\n",
    "                    'Like Count': int(statistics.get('likeCount', 0)),  # Same for like count\n",
    "                    'Comment Count': int(statistics.get('commentCount', 0)),  # And for comment count\n",
    "                    'Favorite Count': int(statistics.get('favoriteCount', 0)),  # Handle favorite count similarly\n",
    "                    'Video Category': snippet.get('categoryId', 'Unknown'),  # We store the category ID as a categorical feature\n",
    "                    'Video Published At': start_date  # Store the date the video was published\n",
    "                })\n",
    "            \n",
    "            # Update the total number of results we've fetched\n",
    "            total_results_fetched += len(video_ids)\n",
    "            \n",
    "            # Check if there is another page of results to fetch\n",
    "            next_page_token = response.get('nextPageToken')\n",
    "            if not next_page_token:\n",
    "                break  # If there are no more pages, we're done here\n",
    "    \n",
    "    # Finally, convert the list of video data into a DataFrame\n",
    "    return pd.DataFrame(video_data)\n",
    "\n",
    "# Now we can use all of this to collect data and save it to a CSV\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"Data Science Tutorial\"  # Set the search term to something relevant (e.g., \"Data Science Tutorial\")\n",
    "    days_to_collect = 30  # Collect data for the last 30 days\n",
    "    max_videos_per_day = 50  # Fetch up to 50 videos in one request\n",
    "    max_total_results_per_day = 150  # Fetch up to 150 videos in total per day (with pagination)\n",
    "    \n",
    "    # Time to collect our video data\n",
    "    df = collect_video_data(query, days=days_to_collect, max_results_per_day=max_videos_per_day, max_total_results_per_day=max_total_results_per_day)\n",
    "    \n",
    "    # Now we save the collected data to a CSV file\n",
    "    df.to_csv('YP_Youtube1_3.csv', index=False)\n",
    "    print(\"Data collection complete. Saved to YP_Youtube1_3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b62008-5c9a-4083-ac34-0b320b413cbc",
   "metadata": {},
   "source": [
    "While gathering the data from YouTube, I encountered an issue where the \"Video Category\" field was returned in numerical format. These numerical category IDs aren't very helpful for understanding what each category represents. I dopn't think the youtube API i am using natively support category names.\n",
    "\n",
    "To make the data more meaningful and readable, we need to map these numerical category IDs to their actual category names (e.g., '27' becomes 'Education', '28' becomes 'Science & Technology', and so on).\n",
    "\n",
    "In the next step, I create a dictionary to map these numerical IDs to their corresponding names, then apply this mapping to the dataset so that the \"Video Category\" field reflects the actual category names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b1f7f15-2903-4a5a-9e2f-8ce5f28d895e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'YP_Youtube1_3.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 26\u001b[0m\n\u001b[1;32m      3\u001b[0m category_mapping \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFilm & Animation\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAutos & Vehicles\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# More categories can be added here if needed, but this should cover the most common ones. I didn't find a better list/map yet if  you do you  can add here\u001b[39;00m\n\u001b[1;32m     22\u001b[0m }\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Now let's load the CSV file that contains our YouTube video data.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# This file has all the video details that we collected earlier.\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYP_Youtube1_3.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Here's where the transformation happens. We're going to replace the category IDs with the actual names.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# First, we make sure the 'Video Category' column is in string format, then we map those IDs to the category names.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVideo Category\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVideo Category\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mmap(category_mapping)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'YP_Youtube1_3.csv'"
     ]
    }
   ],
   "source": [
    "# Now, we're going to create a dictionary to map those category IDs to actual category names. \n",
    "# This way, we can easily understand what each category represents.\n",
    "category_mapping = {\n",
    "    '1': 'Film & Animation',\n",
    "    '2': 'Autos & Vehicles',\n",
    "    '10': 'Music',\n",
    "    '15': 'Pets & Animals',\n",
    "    '17': 'Sports',\n",
    "    '18': 'Short Movies',\n",
    "    '19': 'Travel & Events',\n",
    "    '20': 'Gaming',\n",
    "    '21': 'Videoblogging',\n",
    "    '22': 'People & Blogs',\n",
    "    '23': 'Comedy',\n",
    "    '24': 'Entertainment',\n",
    "    '25': 'News & Politics',\n",
    "    '26': 'Howto & Style',\n",
    "    '27': 'Education',\n",
    "    '28': 'Science & Technology',\n",
    "    '29': 'Nonprofits & Activism'\n",
    "    # More categories can be added here if needed, but this should cover the most common ones. I didn't find a better list/map yet if  you do you  can add here\n",
    "}\n",
    "\n",
    "# Now let's load the CSV file that contains our YouTube video data.\n",
    "# This file has all the video details that we collected earlier.\n",
    "df = pd.read_csv('YP_Youtube1_3.csv')\n",
    "\n",
    "# Here's where the transformation happens. We're going to replace the category IDs with the actual names.\n",
    "# First, we make sure the 'Video Category' column is in string format, then we map those IDs to the category names.\n",
    "df['Video Category'] = df['Video Category'].astype(str).map(category_mapping)\n",
    "\n",
    "# Now that we've updated the data, let's save it to a new CSV file.\n",
    "# This way, the original data stays untouched, and we get a more readable version with category names.\n",
    "df.to_csv('YP_Youtube1_4.csv', index=False)\n",
    "\n",
    "# Finally, let's print a message to confirm that everything worked and the file was saved successfully.\n",
    "print(\"Category mapping completed. Saved to YP_Youtube1_4.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e22c2c6-c344-4c35-903a-479ef0109063",
   "metadata": {},
   "source": [
    "  ## 3. How the Data Will Be Used and Questions of Interest (1%)\n",
    "\n",
    "Once we have the data, we’ll analyze how early engagement metrics relate to a video’s future popularity. Specifically, we’ll focus on:\n",
    "\n",
    "- **Predicting View Counts**: Using the early metrics (views, likes, comments), we’ll try to predict how many views a video will get in the future (like a week later). By building a machine learning model, we’ll look for patterns that can predict future video performance.\n",
    "  \n",
    "- **Identifying Key Metrics**: We’ll also try to figure out which engagement metric has the biggest influence on a video’s popularity. For example, is the number of comments more important than the number of likes?\n",
    "\n",
    "The goal is to create a model that helps us understand and predict how successful a video will be based on its early performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This project dives into the growing trend of using early data to predict video success on YouTube. By collecting and analyzing video data over time, we hope to uncover useful insights for content creators and marketers. Plus, it’s a great way to apply machine learning techniques to real-world data from one of the biggest video platforms in the world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e1af13-3fac-457e-bdd5-3c6c3965dfb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9c6eed-c504-4807-a3d4-c8dc7468dd37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
